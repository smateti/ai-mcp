server:
  port: 8083

spring:
  application:
    name: mcp-chat-app

  # H2 Database for chat history
  datasource:
    url: jdbc:h2:file:./data/mcp-chatdb
    driver-class-name: org.h2.Driver
    username: sa
    password:

  h2:
    console:
      enabled: true
      path: /h2-console

  jpa:
    database-platform: org.hibernate.dialect.H2Dialect
    hibernate:
      ddl-auto: update
    show-sql: false

mcp:
  server:
    url: http://localhost:8082
    execute-endpoint: /mcp/execute

# LLM Provider Configuration
llm:
  # Provider type: ollama-native | ollama-openai | llamacpp
  # - ollama-native: Uses Ollama's native /api/chat endpoint
  # - ollama-openai: Uses OpenAI-compatible /v1/chat/completions endpoint (works with Ollama port 11434 or llama.cpp port 8000)
  # - llamacpp: Uses llama.cpp native /completion endpoint (port 8000, non-OpenAI format - NOT WORKING, use ollama-openai instead)
  provider: ollama-openai

  # Base URL for the LLM provider
  # For Ollama: http://localhost:11434
  # For llama.cpp with OpenAI compatibility: http://localhost:8000
  base-url: http://localhost:8000

  # Model name to use
  chat-model: llama3.1

  # Request timeout in milliseconds
  timeout: 30000

# Tool selection configuration
tool:
  selection:
    confidence:
      high-threshold: 0.8
      low-threshold: 0.5
    max-alternatives: 3
    parameter-timeout-minutes: 5

logging:
  level:
    com.example.chat: DEBUG
    org.springframework.web: INFO
