server:
  port: 8087

spring:
  application:
    name: mcp-chat-app

  # H2 Database for chat history
  datasource:
    url: jdbc:h2:file:./data/mcp-chatdb
    driver-class-name: org.h2.Driver
    username: sa
    password:

  h2:
    console:
      enabled: true
      path: /h2-console

  jpa:
    database-platform: org.hibernate.dialect.H2Dialect
    hibernate:
      ddl-auto: update
    show-sql: false

mcp:
  server:
    url: http://localhost:8082
    execute-endpoint: /mcp/execute

# Category Admin service configuration
category:
  admin:
    url: http://localhost:8085

# LLM Provider Configuration
llm:
  # Provider type: ollama-native | ollama-openai | llamacpp | llamacpp-openai
  # - ollama-native: Uses Ollama's native /api/chat endpoint
  # - ollama-openai: Uses OpenAI-compatible /v1/chat/completions endpoint (works with Ollama port 11434)
  # - llamacpp: Uses llama.cpp native /completion endpoint (non-OpenAI format)
  # - llamacpp-openai: Uses llama.cpp with OpenAI-compatible endpoint (port 8000) - includes system prompt to avoid JSON tool-call format
  provider: llamacpp-openai

  # Base URL for the LLM provider
  # For Ollama: http://localhost:11434
  # For llama.cpp: http://localhost:8000
  base-url: http://localhost:8000

  # Model name to use
  chat-model: llama3.1

  # Request timeout in milliseconds
  timeout: 30000

# Tool selection configuration
tool:
  selection:
    confidence:
      high-threshold: 0.8
      low-threshold: 0.5
    max-alternatives: 3
    parameter-timeout-minutes: 5

logging:
  level:
    com.example.chat: DEBUG
    org.springframework.web: INFO
